# ğŸ§  Language Modeling with RNN, LSTM, and Transformer

This project implements and compares three neural network-based language models â€” **Vanilla RNN**, **LSTM**, and **Transformer** â€” for text generation tasks using custom training data and a SentencePiece tokenizer.

---

## ğŸ“ Project Structure

```
â”œâ”€â”€ data/                    # Raw .txt files and JSONL datasets
â”‚   â”œâ”€â”€ train.jsonl
â”‚   â”œâ”€â”€ test.jsonl
â”‚   â””â”€â”€ *.txt                # Raw text files for tokenizer
â”‚
â”œâ”€â”€ tokenizer/              # Trained SentencePiece model
â”‚   â”œâ”€â”€ spm.model
â”‚   â””â”€â”€ spm.vocab
â”‚
â”œâ”€â”€ model_rnn.py            # RNN model implementation
â”œâ”€â”€ model_lstm.py           # LSTM model implementation
â”œâ”€â”€ model_transformer.py    # Transformer model implementation
â”‚
â”œâ”€â”€ dataset.py              # Dataset class and collation function
â”œâ”€â”€ train.py                # Training script
â”œâ”€â”€ evaluate.py             # Evaluation script (PPL, BLEU)
â”œâ”€â”€ generate.py             # Text generation script
â”‚
â”œâ”€â”€ tokenizer_train.py      # Script for training the SentencePiece tokenizer
â”œâ”€â”€ requirements.txt        # Required Python packages
â””â”€â”€ README.md
```

---

## ğŸ§ª Requirements

Make sure Python 3.8+ is installed. Then run:

```bash
pip install -r requirements.txt
```

---

## ğŸ“ Preparing the Dataset and Tokenizer

1. Put raw `.txt` files into the `data/` folder.
2. Run the tokenizer training script:

```bash
python tokenizer_train.py
```

This generates `tokenizer/spm.model` and `tokenizer/spm.vocab`.

3. Ensure `train.jsonl` and `test.jsonl` are present in the `data/` directory with the following format:

```json
{"prompt": "Once upon a time", "completion": "there was a dragon."}
```

---

## ğŸš€ Training a Model

```bash
python train.py --model lstm     # or "rnn", "transformer"
```

- Model checkpoints and loss plots will be saved to `results/`.

---

## ğŸ“Š Evaluating a Model

```bash
python evaluate.py --model lstm
```

This reports:

- **Perplexity (PPL)**
- **BLEU Score**
- **Total tokens processed**

---

## ğŸ’¬ Generating Text

```bash
python generate.py --model lstm --prompt "The wizard said"
```

Optional flags:

- `--temperature`: Sampling diversity (default: 1.0)
- `--max_len`: Maximum number of tokens to generate (default: 50)

---

## ğŸ›  Supported Models

| Model Type   | Layers | Hidden Size | Notes                    |
|--------------|--------|-------------|--------------------------|
| `rnn`        | 1      | 512         | Vanilla RNN (tanh)       |
| `lstm`       | 2      | 512         | Better for longer memory |
| `transformer`| 6      | 512         | BPE + positional encoding|

---

## ğŸ“ˆ Example Output

```text
ğŸ“ Prompt: 'The wizard said'
ğŸ§  LSTM Completion:
--------------------------------------------------
The wizard said he would return to the castle by morning, and everyone believed him.
--------------------------------------------------
```

---

## ğŸ“Œ Notes

- Models ignore padding tokens during loss calculation.
- Text generation uses greedy sampling via temperature-adjusted multinomial.

---
